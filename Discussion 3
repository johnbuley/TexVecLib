Examples can be found in /test/java/UserTest.

This implementation focuses on the task of processing
and analyzing a large set of documents (10-30gB) in memory.  
In general, this problem lends itself to a distributed 
solution, but focusing on the single-machine case allowed 
me to explore different approaches to data compression.


Performance

Benchmark: 100 mB folder, 2 hyper-threaded CPUs (run on VM)

Branch			master		arrays
---------------------------------------------------------
Ingestion time          3000 ms		3000
---------------------------------------------------------
Time ex. ingestion	500		250
---------------------------------------------------------
Footprint		raw size	10% of raw size	
---------------------------------------------------------


Footprint estimate is based on:

1) Rough sample indicated that 10% of tokens are distinct
2) A String-Integer pair requires 50-70 bytes
3) An n-byte-element can represent a token id and count with
   2*n bytes.


Alternatives I considered for document representation:

1) Using a HashMap<Integer,Integer>, but this is still at
   least 32 bytes per token, because standard Collections
   cannot store primitive types.

2) Enable string deduplication.  This requires 20-24 bytes
   for a String-Integer pair (depending on system architecture, 
   use of pointer compression), carries no guarantee, and 
   increases the GC workload.

3) A 3rd-party library providing primitive collections
   would be worth exploring.  This could be more performant 
   than the byte-array representation, but would still have a 
   larger footprint than a set of 2- and 3-byte-element arrays.

The weakness of the byte-array representation is that retrieval
of a particular token is O(n).  However, this is not an issue
within the context of this problem, because all analysis is
performed with passes over the entire document.

This implementation still uses a global String-Integer HashMap
to index tokens, but the size of this is proportional to the
number of distinct tokens in the corpus, and so should
scale logarithmically.


Design

----
The footprint improvement came from the implementation of
a compressed byte array representation of a document.

A collection is allocated for each SparseDoc to index
the arrays, but this is disposed when compress() is called,
so only one such collection exists at any time.

By default, I use 3-byte-element arrays, but in
DocumentSet.sparsifyDoc() I describe the optimal 
implementation which allows for greater compression.

----
One aspect of the design that may not be intuitive 
is the idea of 'locking' a Corpus.  Locking ensures 
that no new tokens are added, and global occurrence 
data is not modified.  The Corpus is unlocked during 
fitting, and locked during transformation.

----
The previous version had a procedural flavor to
it, so I added some abstraction for extensibility.

Advantages:

   A Corpus object is essentially the set of metadata for
   a DocumentSet data model.  All of the relevant metadata
   is encapsulated in a Corpus, and so this can be
   serialized or written to a database for later use, and
   the DocumentSet may be disposed.  A use case would be
   processing a new set of documents each day, which will
   be added to the existing Corpus.

   The functionality of the library can be expanded more
   easily because the calculation of TfIdfVectorizer has
   been separated from the now-reusable data model.

Disadvantages:

   As the DocumentSet and Corpus are passed around, more
   input checking is required.  Additionally, the Corpus
   in particular can occupy different states, enforced
   by locking.  I chose to handle this with the 
   enum DocumentSetType passed to DocumentSet, but I don't
   know that this would be very intuitive for someone
   else looking over the code.


Tests

Unit-test coverage suffered a bit, because given the time available,
I focused on implementing the improvements I felt were
necessary.  For this version, I relied on a few top-level
tests that aren't atomic.

A better example of a unit-test suite can be found in
the master branch.


Issues

I think the main weakness right now is the input checking
that's done as DocumentSet and Corpus are passed around.  At
the moment, I think there are gaps, and where implemented,
I don't think the input checking would be very intuitive
to someone else.